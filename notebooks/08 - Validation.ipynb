{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../vectorsearch/')\n",
    "import vectorsearch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk_helper\n",
    "from spacy.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_businesses = pd.read_pickle('../input/yelp_academic_dataset_business_SF.pickle')\n",
    "reviews = pd.read_pickle('../output/bar_reviews_cleaned_and_tokenized_SF.pickle')\n",
    "\n",
    "def get_bus_ids_city_state(city, state):\n",
    "    return set(list(df_businesses.business_id[(df_businesses.city.str.lower()==city.lower()) \n",
    "                                     & (df_businesses.state.str.lower()==state.lower())].values))\n",
    "\n",
    "\n",
    "training_users = pickle.load(open('../output/training_users.pickle', 'rb'))\n",
    "test_users     = pickle.load(open('../output/test_users.pickle', 'rb'))\n",
    "\n",
    "# Make the active review set training only \n",
    "review_train = reviews[reviews.user_id.isin(training_users)]\n",
    "review_test = reviews[reviews.user_id.isin(test_users)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import line_profiler\n",
    "import IPython\n",
    "ip = IPython.get_ipython()\n",
    "ip.define_magic('lprun', line_profiler.magic_lprun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " progress 16750 of 16752 TEST SET DONE\n",
      "TRAIN SET DONE\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from gensim.models import doc2vec\n",
    "doc2vec_model = doc2vec.Doc2Vec.load('../output/doc2vec_bars_100.model')\n",
    "\n",
    "def GetDocumentRank(review_text, real_bus_id, bus_ids_in_city_state):\n",
    "    '''\n",
    "    Given a document review, search businesses in that city, state and return the rank from the top of the list. \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    top_n = 1000 # Number of topics to choose for top of list. \n",
    "    rev_topic = np.array(vectorsearch.GetDocTopic(review_text, n_jobs=1),)\n",
    "    \n",
    "    \n",
    "    #real_business = df_businesses[df_businesses.business_id==real_bus_id]\n",
    "    #city, state = real_business.city.values[0], real_business.state.values[0]\n",
    "    \n",
    "    # Lookup only businesses in the city/state\n",
    "    #bus_ids_in_city_state = get_bus_ids_city_state(city.strip().lower(), state.strip().lower())\n",
    "    # Get the topic vector for the review \n",
    "    # topic_listings = [\" \".join(vectorsearch.GetTopicWords(topic, ))  for topic in top_n_topics]\n",
    "    # Find similarity with the businesses of interst (in the city state)\n",
    "    top_bus_id, top_bus_sim = vectorsearch.FindBusinessSimilarityLDA(rev_topic, \n",
    "                                                                     business_ids=bus_ids_in_city_state,\n",
    "                                                                     top_n=top_n, method='Hel')\n",
    "    \n",
    "    # Length of cleaned and tokenized document for segmentation.\n",
    "    doc_length = vectorsearch.GetDocLength(review_text)\n",
    "    # where in the results was the true business?\n",
    "    try:\n",
    "        rank = np.argwhere(top_bus_id==real_bus_id)[0][0]+1\n",
    "    except:\n",
    "        rank = float('NaN')\n",
    "    return rank, doc_length\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def GetDocumentRank_doc2vec(review_text, real_bus_id, bus_ids_in_city_state):\n",
    "    '''\n",
    "    Given a document review, search businesses in that city, state and return the rank from the top of the list. \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    top_n = -1 # Number of topics to choose for top of list. \n",
    "    \n",
    "    # Clean and tokenize....\n",
    "    cleaned_review = nltk_helper.clean_and_tokenize(review_text,)\n",
    "    # Chain the sentences together for LDA BOW approach.  word2vec requires sentence sep...\n",
    "    cleaned_bow = list(chain.from_iterable(cleaned_review))\n",
    "    \n",
    "    doc_length = len(cleaned_bow)\n",
    "    # Get doc_vector for input review.... \n",
    "    doc_vec = doc2vec_model.infer_vector(cleaned_bow,)\n",
    "    # Get similarity to all businesses \n",
    "    top_bus_ids, top_sims = zip(*doc2vec_model.docvecs.most_similar(positive=[doc_vec,], \n",
    "                                           topn=len(doc2vec_model.docvecs.doctags)))\n",
    "    # find ranking out of those that are in the city/state\n",
    "    df = pd.DataFrame.from_dict(dict(bids=top_bus_ids))\n",
    "    # Find the entries in the relevant city\n",
    "    df = df[df.bids.isin(bus_ids_in_city_state)]\n",
    "    try:\n",
    "        rank = np.argwhere(df.bids==real_bus_id)[0][0] + 1\n",
    "    except:\n",
    "        rank = float('NaN')\n",
    "    \n",
    "    return rank, doc_length\n",
    "\n",
    "\n",
    "def get_bus_ids_city_state(city, state):\n",
    "    bids =  set(list(df_businesses.business_id[(df_businesses.city==city) \n",
    "                                     & (df_businesses.state==state)].values))\n",
    "    return bids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def RunValidation(review_set, city, state, method='LDA'):\n",
    "    ranks, doc_lengths = [], [] \n",
    "    bids = get_bus_ids_city_state(city, state)    \n",
    "    validation_set = review_set[review_set.business_id.isin(bids)]    \n",
    "\n",
    "    for i_rev in range(len(validation_set)):\n",
    "        #print i_rev\n",
    "        row = validation_set.iloc[i_rev]\n",
    "        #try:\n",
    "        if (i_rev%50==0): \n",
    "            print '\\r progress %i of'%i_rev, len(validation_set), \n",
    "        \n",
    "        if method=='LDA':\n",
    "            rank, doc_length = GetDocumentRank(row.text, row.business_id, bids)\n",
    "            \n",
    "        elif method=='doc2vec':\n",
    "            rank, doc_length = GetDocumentRank_doc2vec(row.text, row.business_id, bids)\n",
    "        ranks.append(rank)\n",
    "        doc_lengths.append(doc_length)\n",
    "        #except:\n",
    "        #    pass\n",
    "\n",
    "    return bids, ranks, doc_lengths\n",
    "\n",
    "\n",
    "#%load_ext line_profiler\n",
    "#%prun RunValidation(review_test, \"San Francisco\", \"CA\")\n",
    "\n",
    "# -----------------------------------\n",
    "# LDA results\n",
    "# test_results = RunValidation(review_test, \"San Francisco\", \"CA\")\n",
    "# print \"TEST SET DONE\"\n",
    "# train_results = RunValidation(review_train[:100], \"San Francisco\", \"CA\")\n",
    "# print \"TRAIN SET DONE\"\n",
    "\n",
    "\n",
    "# doc2vec results\n",
    "\n",
    "test_results = RunValidation(review_test.iloc[:], \"San Francisco\", \"CA\", method='doc2vec')\n",
    "print \"TEST SET DONE\"\n",
    "train_results = RunValidation(review_train[:100], \"San Francisco\", \"CA\")\n",
    "print \"TRAIN SET DONE\"\n",
    "# -----------------------------------\n",
    "# doc2vec results\n",
    "\n",
    "results = {'test_results':test_results, 'train_results':train_results}\n",
    "with open('../output/validation_results_doc2vec_100_SF_withChunk.pickle', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import OrderedDict\n",
    "result_sets = OrderedDict(\n",
    "    (('LDA-Cos-NNC', pickle.load(open('../output/validation_results_LDA_SF_noChunk.pickle', 'rb'))),\n",
    "   #('LDA+NC Cos', pickle.load(open('../output/validation_results_LDA_SF_withChunk.pickle', 'rb'))),\n",
    "   ('LDA-Hel', pickle.load(open('../output/validation_results_LDA_SF_withChunk_Hel.pickle', 'rb'))),\n",
    "   ('doc2vec50', pickle.load(open('../output/validation_results_doc2vec_50_SF_withChunk.pickle', 'rb'))),\n",
    "   ('doc2vec100', pickle.load(open('../output/validation_results_doc2vec_100_SF_withChunk.pickle', 'rb')))))\n",
    "           \n",
    "\n",
    "\n",
    "# ranks_train = np.array(train_results[1])\n",
    "# n_businesses = len(train_results[0])\n",
    "# doc_lengths_train = np.array(train_results[2])\n",
    "\n",
    "\n",
    "def GetCDFDocLength(min_length):\n",
    "    ranks_tmp = ranks[doc_lengths>min_length]\n",
    "    CDF = np.array([np.sum(ranks_tmp<i) for i in range(n_businesses)])/float(len(ranks_tmp))\n",
    "    return CDF\n",
    "\n",
    "\n",
    "colors={'LDA-Cos-NNC':'steelblue','LDA-Hel':'darkseagreen', 'doc2vec50':'goldenrod', 'doc2vec100':'firebrick'}\n",
    "\n",
    "for key, result_set in result_sets.items():  \n",
    "    ranks = np.array(result_set['test_results'][1])\n",
    "    n_businesses = len(result_set['test_results'][0])\n",
    "    doc_lengths = np.array(result_set['test_results'][2])\n",
    "    print \"Num NaN's\", np.sum(np.isnan(ranks))\n",
    "    doc_lengths = doc_lengths[~np.isnan(ranks)]\n",
    "    ranks = ranks[~np.isnan(ranks)]\n",
    "\n",
    "    for min_length in [0,50,]:\n",
    "        CDF = GetCDFDocLength(min_length)\n",
    "        alpha=1\n",
    "        label=key\n",
    "        linestyle=\"-\"\n",
    "        linewidth=1\n",
    "        if min_length==50:\n",
    "            print 'here'\n",
    "            alpha=1\n",
    "            label=None\n",
    "            linestyle=\":\"\n",
    "        if key=='doc2vec100':\n",
    "            \n",
    "        linewidth=1\n",
    "        plt.plot(range(n_businesses),CDF, label=label, color=colors[key], alpha=alpha, \n",
    "                 linestyle=linestyle, linewidth=linewidth)\n",
    "\n",
    "\n",
    "#     CDF = np.array([np.sum(ranks_train<i) for i in range(n_businesses)])/float(len(ranks_train))\n",
    "#     plt.plot(range(n_businesses), CDF, label=r'LDA Training Set', linestyle=':')\n",
    "\n",
    "\n",
    "\n",
    "# Random Choices....\n",
    "random_choices = np.random.randint(1, n_businesses, len(ranks))\n",
    "CDF = np.array([np.sum(random_choices<i) for i in range(n_businesses)])/float(len(random_choices))\n",
    "plt.plot(range(n_businesses), CDF, label='Random Choices', linestyle='--')\n",
    "\n",
    "plt.vlines(10, 0, .5, alpha=1, linestyle='--' )\n",
    "plt.hlines(.5, 1, 1e1, alpha=1, linestyle='--' )\n",
    "\n",
    "plt.legend(loc=2, frameon=False, handlelength=3)\n",
    "plt.xscale('log')\n",
    "plt.xlim(1,1e3)\n",
    "plt.xlabel(r'Rank Cutoff $r_{\\rm max}$')\n",
    "plt.ylabel(r'CDF(r$<r_{\\rm max}$)')\n",
    "plt.title('Review Match Accuracy: San Francisco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import yelp\n",
    "\n",
    "# yelp_api = yelp.Api(consumer_key='SddBgAA4qAbxz-UhAS0D7A',\n",
    "#                     consumer_secret='z-d2Q7yB963kMcusvLVm9I2Jylg',\n",
    "#                     access_token_key='KL574SXMJijEoO1_poHKhm6gHnSbwNlS',\n",
    "#                     access_token_secret='bn7i1wGZ8hNK5MLuykvMwfpEB4g')\n",
    "\n",
    "\n",
    "# def SearchYelpReviews(review, location, real_bus_URL):\n",
    "#     search_results = yelp_api.Search(term=review, location=location, \n",
    "# #                                  cll=str(lat)+','+str(lon), radius_filter=radius, \n",
    "#                                  limit=20, category_filter='bars',) # location and search term are required\n",
    "    \n",
    "#     for i, bus in enumerate(search_results.businesses):\n",
    "#         if bus.url == real_bus_URL:\n",
    "#             return i+1\n",
    "#         else:\n",
    "#             return 999\n",
    "\n",
    "# import nltk_helper\n",
    "# from itertools import chain \n",
    "\n",
    "# def RunValidationYelp(review_set, city, state):\n",
    "    \n",
    "#     ranks, doc_lengths = [], [] \n",
    "#     bids = get_bus_ids_city_state(city, state)\n",
    "#     validation_set = review_set[review_set.business_id.isin(bids)]\n",
    "    \n",
    "    \n",
    "#     for i_rev in range(50):#len(validation_set)):\n",
    "#         row = validation_set.iloc[i_rev]\n",
    "\n",
    "#         if (i_rev%50==0): \n",
    "#             print '\\r progress %i of'%i_rev, len(validation_set), \n",
    "\n",
    "#         real_URL = df_businesses.URL[row.business_id==df_businesses.business_id].values[0]\n",
    "#         text_cleaned = nltk_helper.clean_and_tokenize(row.text)\n",
    "#         text_cleaned = \",\".join(list(chain.from_iterable(text_cleaned))[:8])\n",
    "#         #print text_cleaned\n",
    "#         rank = SearchYelpReviews(text_cleaned,\n",
    "#                                  location=\", \".join([city, state]), real_bus_URL=real_URL )\n",
    "#         ranks.append(rank)\n",
    "#     return ranks\n",
    "        \n",
    "        \n",
    "# ranks = RunValidationYelp(review_test, 'San Francisco', 'CA')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #search_results.businesses[0].url = search_results.businesses[0].url.split('?')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1035\n"
     ]
    }
   ],
   "source": [
    "print len(set(list(df_businesses.business_id[(df_businesses.city.str.lower()=='San Francisco'.lower()) \n",
    "                                     & (df_businesses.state.str.lower()=='ca')].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1035"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_bus_ids_city_state(city, state):\n",
    "    return set(list(df_businesses.business_id[(df_businesses.city.str.lower()==city.lower()) \n",
    "                                     & (df_businesses.state.str.lower()==state.lower())].values))\n",
    "\n",
    "len(get_bus_ids_city_state('San Francisco', 'CA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5620, 30)\n",
      "[ 0.          0.35656793  0.8043929  ...,  0.67698329  0.73591688\n",
      "  0.97432134]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "bus_lda_vecs = pd.read_pickle('../output/business_LDA_vectors.pickle')\n",
    "top_vecs = np.vstack(bus_lda_vecs.topic_vector.values)\n",
    "\n",
    "top_vecs = (top_vecs.T/np.sum(top_vecs, axis=1)).T\n",
    "print top_vecs.shape\n",
    "\n",
    "\n",
    "def hel(p,q):\n",
    "    return np.linalg.norm(np.sqrt(p) - np.sqrt(q), axis=1) / np.sqrt(2)\n",
    "\n",
    "\n",
    "print hel(top_vecs[0], top_vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'XVEKceEWZbSz1P6Roa8sCA',\n",
       "  u'uVqODB0tnRtp3kD9cDeNWw',\n",
       "  u'GkYUFGLqCfsnLk2zpxIx0w',\n",
       "  u'6SoHvHM0NJq35x4bCdfc3A'),\n",
       " (0.6897120475769043,\n",
       "  0.6226238012313843,\n",
       "  0.6200392842292786,\n",
       "  0.602697491645813)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(*[(u'XVEKceEWZbSz1P6Roa8sCA', 0.6897120475769043), (u'uVqODB0tnRtp3kD9cDeNWw', 0.6226238012313843), (u'GkYUFGLqCfsnLk2zpxIx0w', 0.6200392842292786), (u'6SoHvHM0NJq35x4bCdfc3A', 0.602697491645813)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
