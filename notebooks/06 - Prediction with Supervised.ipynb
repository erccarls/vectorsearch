{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Supervised approach to rating predction. \n",
    "\n",
    "In this notebook, we feed the LDA and word2vec predictions into a supervised algorithm in order to predict the rating differential.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get the rating differential..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from itertools import chain\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_pickle('../output/bar_reviews_cleaned_and_tokenized.pickle')\n",
    "\n",
    "training_users = pickle.load(open('../output/training_users.pickle', 'rb'))\n",
    "test_users     = pickle.load(open('../output/test_users.pickle', 'rb'))\n",
    "\n",
    "# Make the active review set training only \n",
    "review_train = reviews[reviews.user_id.isin(training_users)]\n",
    "review_test = reviews[reviews.user_id.isin(test_users)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the LDA Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../vectorsearch/')\n",
    "import LDA\n",
    "\n",
    "# Load the LDA models for businesses and companies\n",
    "review_lda = LDA.LoadLDAModel('../output/LDA_model_reviews.pickle')\n",
    "bus_lda = LDA.LoadLDAModel('../output/LDA_model_bus.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize the docs_reviews for use as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GenerateInputOutput(review_set, lda_model):\n",
    "    '''\n",
    "    Given a list of reviews...\n",
    "    \n",
    "    Returns \n",
    "        docs_reviews : list\n",
    "            document string for each review\n",
    "        bus_ids : list \n",
    "            ids for each business\n",
    "        rev_diff: list \n",
    "            difference between the user rating and average rating \n",
    "    '''\n",
    "\n",
    "    # For each business, generate list of average reviews...\n",
    "    avg_reviews = review_set.groupby('business_id').mean()['stars']\n",
    "\n",
    "    # Get the differential for each review \n",
    "    rev_diff = map(lambda (bus_id, stars): stars - avg_reviews[bus_id], \n",
    "                   zip(review_set.business_id.values, review_set.stars.values) )\n",
    "    # Get the review text\n",
    "    docs_reviews = [\" \".join(list(chain.from_iterable(rev))) for rev in review_set.cleaned_tokenized.values]\n",
    "    # Convert the documents into vectorized form as input to LDA. \n",
    "    # *These are the LDA features* \n",
    "    doc_LDA_topic_vectors = lda_model.get_doc_topics(docs_reviews)\n",
    "    \n",
    "    \n",
    "    # List of business ids for each review\n",
    "    bus_ids = review_set.business_id.values\n",
    "    return doc_LDA_topic_vectors, bus_ids, rev_diff\n",
    "\n",
    "# Generate for test and training data. \n",
    "doc_LDA_topic_vectors_train, bus_ids_train, rev_diff_train = GenerateInputOutput(review_train, review_lda)\n",
    "doc_LDA_topic_vectors_test, bus_ids_test, rev_diff_test = GenerateInputOutput(review_test, review_lda)\n",
    "\n",
    "# Normalize the topic vectors...\n",
    "doc_LDA_topic_vectors_train = [top/np.sqrt(np.dot(top,top)) for top in doc_LDA_topic_vectors_train]\n",
    "doc_LDA_topic_vectors_test = [top/np.sqrt(np.dot(top,top)) for top in doc_LDA_topic_vectors_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the business topic reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# This is business ids corresponding to the business LDA vectors\n",
    "bus_lda_ids = pickle.load(open('../output/bus_ids_bars_LDA.pickle', 'rb'))\n",
    "\n",
    "# pd.dataframe('bus_id', 'topic_vector')\n",
    "\n",
    "\n",
    "# # for each review, lookup the corresponding business topic vector \n",
    "# for bus_id in bus_ids_train[:2]:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The topic vector for a given business is given by this dataframe. \n",
    "bus_lda_ids = pickle.load(open('../output/bus_ids_bars_LDA.pickle', 'rb'))\n",
    "bus_vectors = pd.DataFrame()\n",
    "bus_vectors['business_id'] = bus_lda_ids\n",
    "transformed = bus_lda.lda.fit_transform(bus_lda.tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bus_vectors['topic_vector'] = [bus_topic_vec for bus_topic_vec in transformed]\n",
    "\n",
    "normed_topic_vecs = map(lambda topic_vec: topic_vec/sqrt(np.dot(topic_vec, topic_vec)),\n",
    "                        bus_vectors.topic_vector) \n",
    "bus_vectors.topic_vector = normed_topic_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bus_vectors.to_pickle('../output/business_LDA_vectors.pickle')\n",
    "#print bus_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find business topic vector each review \n",
    "review_bus_vectors_train = pd.DataFrame({'business_id':bus_ids_train})\n",
    "review_bus_vectors_train = pd.merge(review_bus_vectors_train, bus_vectors, how='left', on='business_id')\n",
    "# Same for test set. \n",
    "review_bus_vectors_test = pd.DataFrame({'business_id':bus_ids_test})\n",
    "review_bus_vectors_test = pd.merge(review_bus_vectors_test, bus_vectors, how='left', on='business_id')\n",
    "\n",
    "\n",
    "# Some businesses don't have topic vectors... drop those.  \n",
    "# Need to also drop them from the relative reviews\n",
    "blacklist_train = []\n",
    "for i, rev in enumerate(review_bus_vectors_train.topic_vector.values):\n",
    "    if np.isnan(rev).any():        \n",
    "        blacklist_train.append(i)\n",
    "        \n",
    "        \n",
    "blacklist_test = []\n",
    "for i, rev in enumerate(review_bus_vectors_test.topic_vector.values):\n",
    "    if np.isnan(rev).any():\n",
    "        blacklist_test.append(i)\n",
    "        \n",
    "        \n",
    "review_bus_vectors_train['review_diff'] = rev_diff_train\n",
    "review_bus_vectors_test['review_diff'] = rev_diff_test\n",
    "\n",
    "review_bus_vectors_train['review_topic_vector'] = [doc for doc in doc_LDA_topic_vectors_train]\n",
    "review_bus_vectors_test['review_topic_vector'] = [doc for doc in doc_LDA_topic_vectors_test]\n",
    "\n",
    "# Drop the blacklisted businesses....\n",
    "review_bus_vectors_train = review_bus_vectors_train.drop(review_bus_vectors_train.index[blacklist_train])\n",
    "review_bus_vectors_test  = review_bus_vectors_test.drop(review_bus_vectors_test.index[blacklist_test])\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack the input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186752, 40)\n",
      "(186752,)\n"
     ]
    }
   ],
   "source": [
    "X_TRAIN =  np.append(np.vstack(review_bus_vectors_train.review_topic_vector.values),\n",
    "                     np.vstack(review_bus_vectors_train.topic_vector.values), axis=1)\n",
    "Y_TRAIN =  review_bus_vectors_train.review_diff.values\n",
    "\n",
    "X_TEST  =  np.append(np.vstack(review_bus_vectors_test.review_topic_vector.values),\n",
    "                     np.vstack(review_bus_vectors_test.topic_vector.values), axis=1)\n",
    "Y_TEST  =  review_bus_vectors_test.review_diff.values\n",
    "\n",
    "\n",
    "np.save('../output/bar_X_TRAIN.npy', X_TRAIN)\n",
    "np.save('../output/bar_Y_TRAIN.npy', Y_TRAIN)\n",
    "\n",
    "np.save('../output/bar_X_TEST.npy', X_TEST)\n",
    "np.save('../output/bar_Y_TEST.npy', Y_TEST)\n",
    "\n",
    "\n",
    "\n",
    "print X_TRAIN.shape\n",
    "print Y_TRAIN.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all businesses that were reviewed by a user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function we want to optimize is the L2 loss on the difference between the users actual rating minus the average (this is $f$) and the predicted rating differential for the business $J = (f-\\hat{f})^2$.   In contrast to preducting the rating directly, this will allow the supervised alogrithm to try and predict deviations from the average behavior.   Hence we can try to find underdogs, or places that may not be rated well, but have a high probability of being liked by the user.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b694d49c68a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mRMS_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMSE_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetRMS_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_TRAIN\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_TRAIN\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[0mRMS_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMSE_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetRMS_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_TEST\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_TEST\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'RMS Training Error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRMS_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_samples' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "\n",
    "def RunRFClassifier(n_samples, X, Y, **kwargs):\n",
    "    RF = RandomForestRegressor(**kwargs)\n",
    "    RF.fit(X[:n_samples], Y[:n_samples])\n",
    "    return RF\n",
    "    \n",
    "\n",
    "\n",
    "def getRMS_error(RF, X, Y): \n",
    "    Y_predict = RF.predict(X)\n",
    "    MSE = (Y-Y_predict)**2\n",
    "    RMS_errors = np.sqrt(np.average(MSE))\n",
    "    return RMS_errors, MSE \n",
    "\n",
    "\n",
    "RF_settings = { 'n_estimators':500, \n",
    "                'max_depth':10, \n",
    "                'min_samples_split':2, \n",
    "                'min_samples_leaf':5,\n",
    "                'min_weight_fraction_leaf':0.0,\n",
    "                'max_features':'auto', \n",
    "                'max_leaf_nodes':None,\n",
    "                'bootstrap':True, \n",
    "                'oob_score':True,\n",
    "                'n_jobs':12,\n",
    "                'random_state':0}\n",
    "\n",
    "\n",
    "RF = RunRFClassifier(20000, X_TRAIN, Y_TRAIN, **RF_settings)\n",
    "\n",
    "\n",
    "RMS_train, MSE_train = getRMS_error(RF, X_TRAIN[:n_samples], Y_TRAIN[:n_samples])\n",
    "RMS_test, MSE_test = getRMS_error(RF, X_TEST[:n_samples], Y_TEST[:n_samples])\n",
    "print 'RMS Training Error', RMS_train\n",
    "print 'RMS Test Error', RMS_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0,4,41)\n",
    "\n",
    "plt.hist(Y_TRAIN[:n_samples]**2, bins, histtype='step', label='Random')\n",
    "plt.hist(MSE_train, bins, histtype='step', label='RF Train')\n",
    "plt.hist(MSE_test, bins, histtype='step', label='RF Test',)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name MLPRegressor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-273-f2db13262455>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMLPRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m MLP = MLPRegressor(hidden_layer_sizes=(50, ), activation='relu', algorithm='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, \n\u001b[0;32m      4\u001b[0m                                     validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name MLPRegressor"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "MLP = MLPRegressor(hidden_layer_sizes=(50, ), activation='relu', algorithm='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, \n",
    "                                    validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2000)\n"
     ]
    }
   ],
   "source": [
    "print np.dot(X_TRAIN[:2000],X_TRAIN[:2000].T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.41421356237\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print np.sqrt(np.dot(X_TEST[i],X_TEST[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
