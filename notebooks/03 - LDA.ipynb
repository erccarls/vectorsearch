{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "%load_ext autoreload\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the bar review dataset \n",
    "review = pd.read_pickle('../output/bar_reviews_cleaned_and_tokenized.pickle')\n",
    "\n",
    "# Drop 20% of the users from the dataset for testing\n",
    "user_set = list(set(review.user_id.values[:]))\n",
    "random.seed(0)\n",
    "shuffle(user_set) # Randomize \n",
    "n_users = float(len(user_set))\n",
    "\n",
    "user_set_training = user_set[:int(n_users*float(0.8))]\n",
    "with open('../output/training_users.pickle', 'wb') as f: \n",
    "    pickle.dump(user_set_training, f)\n",
    "    \n",
    "# Save a test set\n",
    "test_users = user_set[int(n_users*float(0.8)):]\n",
    "with open('../output/test_users.pickle', 'wb') as f: \n",
    "    pickle.dump(test_users, f)\n",
    "    \n",
    "# Make the active review set training only \n",
    "review = review[review.user_id.isin(user_set_training)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the documents by (i) business, (ii) users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fraction Processed 0.999782040105"
     ]
    }
   ],
   "source": [
    "# This is for review level not business level \n",
    "# docs = [\" \".join(list(chain.from_iterable(l))) for l in review.cleaned_tokenized.iloc[:]]\n",
    "\n",
    "n_reviews = -1 # all of them... \n",
    "# Flatten the reviews, so each review is just a single list of words.\n",
    "reviews_merged_bus = OrderedDict()\n",
    "business_set = set(review.business_id.values[:n_reviews])\n",
    "for i_bus, bus_id in enumerate(business_set):\n",
    "    if ((i_bus%2)==0):\n",
    "        print ('\\r Fraction Processed',float(i_bus+1)/len(business_set), end=\"\") \n",
    "    # This horrible line first collapses each review of a corresponding business into a list\n",
    "    # of lists, and then collapses the list of sentences to a long list of words\n",
    "    reviews_merged_bus[bus_id] = \" \".join(list(chain.from_iterable( \n",
    "                                    chain.from_iterable( review.cleaned_tokenized[review.business_id==bus_id] ))))    \n",
    "docs_bus = reviews_merged_bus.values()\n",
    "\n",
    "with open('../output/docs_bars_bus.pickle', 'wb') as f: \n",
    "    pickle.dump(docs_bus, f)\n",
    "\n",
    "with open('../output/bus_ids_bars_LDA.pickle', 'wb') as f: \n",
    "    pickle.dump(reviews_merged_bus.keys(), f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note that this section merges all reviews by the same person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fraction Processed 0.999817553761\n",
      "Merging Done...\n"
     ]
    }
   ],
   "source": [
    "# Flatten the reviews, so each review is just a single list of words.\n",
    "# reviews_merged_user = OrderedDict()\n",
    "\n",
    "# user_set = list(set(review.user_id.values[:n_reviews]))\n",
    "# n_users = float(len(user_set))\n",
    "# for i_user, user_id in enumerate(user_set[:]):\n",
    "#     if ((i_user%50)==0):\n",
    "#         print ('\\r Fraction Processed',float(i_user+1)/n_users, end=\"\") \n",
    "#     # This horrible line first collapses each review of a corresponding user reviews into a list\n",
    "#     # of lists, and then collapses the list of sentences to a long list of words\n",
    "#     reviews_merged_user[user_id] = \" \".join(list(chain.from_iterable( \n",
    "#                                     chain.from_iterable( review.cleaned_tokenized[review.user_id==user_id] ))))    \n",
    "# docs_users = reviews_merged_user.values()\n",
    "# print()\n",
    "# print(\"Merging Done...\")\n",
    "\n",
    "# with open('../output/docs_bars_users.pickle', 'wb') as f: \n",
    "#     pickle.dump(docs_users, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Flatten the reviews, so each review is just a single list of words.\n",
    "docs_reviews = [\" \".join(list(chain.from_iterable(rev))) for rev in review.cleaned_tokenized.values[:n_reviews]]\n",
    "\n",
    "with open('../output/docs_reviews.pickle', 'wb') as f: \n",
    "    pickle.dump(docs_reviews, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Across Bars and Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2 \n",
    "import sys\n",
    "sys.path.append('../vectorsearch/')\n",
    "import LDA\n",
    "reload(LDA)\n",
    "n_topics=20 \n",
    "n_features=10000\n",
    "max_df=.75\n",
    "min_df=2\n",
    "max_iter=5\n",
    "alpha=7./n_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 10.749s.\n",
      "Fitting LDA models with tf features, n_samples=4588 and n_features=10000...\n",
      "done in 194.396s.\n"
     ]
    }
   ],
   "source": [
    "# Train the bar set over businesses\n",
    "doc_users = pickle.load(open('../output/docs_bars_users.pickle', 'rb'))\n",
    "\n",
    "lda_bus = LDA.LDA(alpha=alpha, n_topics=n_topics, n_features=n_features, max_df=max_df, min_df=min_df, max_iter=max_iter,)\n",
    "lda_bus.vectorizecounts(docs_bus)\n",
    "lda_bus.fitLDA()\n",
    "LDA.SaveLDAModel('../output/LDA_model_bus.pickle', lda_bus)\n",
    "# Now can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 10.863s.\n",
      "Fitting LDA models with tf features, n_samples=82216 and n_features=10000...\n",
      "done in 653.609s.\n"
     ]
    }
   ],
   "source": [
    "# Train the bar set over users\n",
    "\n",
    "# doc_users = pickle.load(open('../output/docs_bars_users.pickle', 'rb'))\n",
    "# lda_user = LDA.LDA(n_topics=n_topics, n_features=n_features, max_df=max_df, min_df=min_df, max_iter=max_iter,)\n",
    "# lda_user.vectorizecounts(docs_users)\n",
    "# lda_user.fitLDA()\n",
    "# LDA.SaveLDAModel('../output/LDA_model_user.pickle', lda_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 11.443s.\n",
      "Fitting LDA models with tf features, n_samples=186751 and n_features=10000...\n",
      "done in 496.987s.\n"
     ]
    }
   ],
   "source": [
    "# Train the bar set over users\n",
    "lda_reviews = pickle.load(open('../output/docs_reviews.pickle', 'rb'))\n",
    "lda_reviews = LDA.LDA(alpha=alpha, n_topics=n_topics, n_features=n_features, max_df=max_df, min_df=min_df, max_iter=max_iter,)\n",
    "lda_reviews.vectorizecounts(docs_reviews)\n",
    "lda_reviews.fitLDA()\n",
    "LDA.SaveLDAModel('../output/LDA_model_reviews.pickle', lda_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# doc_users = pickle.load(open('../output/docs_bars_users.pickle', 'rb'))\n",
    "# lda_user = LDA.LDA(n_topics=n_topics, n_features=n_features, max_df=max_df, min_df=min_df, max_iter=max_iter,)\n",
    "# lda_user.vectorizecounts(docs_users)\n",
    "# lda_user.fitLDA()\n",
    "# LDA.SaveLDAModel('../output/LDA_model_user.pickle', lda_user)\n",
    "\n",
    "\n",
    "#lda_bus.print_top_words(10)\n",
    "\n",
    "#.get_doc_topics(doc_users[10:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 20)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#print(lda_user.tf_vectorizer.fit_transform( docs_users[:10] ))\n",
    "\n",
    "lda_reviews.get_doc_topics(docs_reviews[:3]).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualizationlda_reviews.get_doc_topics(doc_reviews[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6dcbdbb2d26e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mvis_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-6dcbdbb2d26e>\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(docs, vect, lda, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-6dcbdbb2d26e>\u001b[0m in \u001b[0;36m_extract_data\u001b[1;34m(docs, vect, lda)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     return lda,vect, dict(\n\u001b[1;32m---> 15\u001b[1;33m                       \u001b[0mdoc_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                       \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                       \u001b[0mterm_frequency\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvected\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'str'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pandas as pd\n",
    "import funcy as fp\n",
    "from pyLDAvis import prepare as vis_prepare\n",
    "\n",
    "def _extract_data(docs, vect, lda):\n",
    "    #LDA scikit-learn implementation seems to have buggy code.\n",
    "    #Topic_term_dists and doc_topic_dists isn't accummulated to 1.\n",
    "    #Hence norm function implemented to normalize the distributions.\n",
    "    norm = lambda data: pd.DataFrame(data).div(data.sum(1),axis=0).values\n",
    "    vected = vect.fit_transform(docs)\n",
    "    doc_topic_dists = norm(lda.fit_transform(vected))\n",
    "    \n",
    "    return lda,vect, dict(\n",
    "                      doc_lengths = docs.str.len(),\n",
    "                      vocab = vect.get_feature_names(),\n",
    "                      term_frequency = vected.sum(axis=0).tolist()[0],\n",
    "                      topic_term_dists = norm(lda.components_),\n",
    "                      doc_topic_dists = doc_topic_dists)\n",
    "\n",
    "def prepare(docs, vect, lda, **kwargs):\n",
    "    \"\"\"Create Prepared Data from sklearn's vectorizer and Latent Dirichlet\n",
    "    Application.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs : Pandas Series.\n",
    "        Documents to be passed as an input.\n",
    "    vect : Scikit-Learn Vectorizer (CountVectorizer,TfIdfVectorizer).\n",
    "        vectorizer to convert documents into matrix sparser\n",
    "    lda  : sklearn.decomposition.LatentDirichletAllocation.\n",
    "        Latent Dirichlet Allocation\n",
    "\n",
    "    **kwargs: Keyword argument to be passed to pyLDAvis.prepare()\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    prepared_data : PreparedData\n",
    "          the data structures used in the visualization\n",
    "\n",
    "\n",
    "    Example\n",
    "    --------\n",
    "    For example usage please see this notebook:\n",
    "    http://nbviewer.ipython.org/github/bmabey/pyLDAvis/blob/master/notebooks/sklearn.ipynb\n",
    "\n",
    "    See\n",
    "    ------\n",
    "    See `pyLDAvis.prepare` for **kwargs.\n",
    "    \"\"\"\n",
    "    \n",
    "    opts = fp.merge(_extract_data(docs, vect, lda)[2], kwargs)\n",
    "\n",
    "    return vis_prepare(**opts)\n",
    "\n",
    "vis_data = prepare(docs, tf_vectorizer, lda)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import NMF\n",
    "\n",
    "\n",
    "# n_samples = 100000\n",
    "# n_features = 10000\n",
    "# n_topics = 6\n",
    "# n_top_words = 20\n",
    "\n",
    "\n",
    "# # # Use tf-idf features for NMF.\n",
    "# print(\"Extracting tf-idf features for NMF...\")\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features=n_features)\n",
    "\n",
    "\n",
    "# t0 = time()\n",
    "# tfidf = tfidf_vectorizer.fit_transform(review_flatten[:n_samples])\n",
    "# print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "# # Fit the NMF model\n",
    "# print(\"Fitting the NMF model with tf-idf features,\"\n",
    "#       \"n_samples=%d and n_features=%d...\"\n",
    "#       % (n_samples, n_features))\n",
    "# t0 = time()\n",
    "# nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "# print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# print(\"\\nTopics in NMF model:\")\n",
    "# tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "# print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# Load the bar review dataset \n",
    "review = pd.read_pickle('../output/bar_reviews_cleaned_and_tokenized.pickle')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
