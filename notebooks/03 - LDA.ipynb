{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "%load_ext autoreload\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the bar review dataset \n",
    "review = pd.read_pickle('../output/bar_reviews_cleaned_and_tokenized_SF.pickle')\n",
    "review.head(5)\n",
    "df_businesses = pd.read_pickle('../input/yelp_academic_dataset_business_SF.pickle')\n",
    "city_state_list = list(set([df_businesses.city.iloc[i_city]+', '+df_businesses.state.iloc[i_city] for i_city, city in enumerate(df_businesses.city)]))[1:]\n",
    "import pickle\n",
    "pickle.dump(city_state_list, open('../output/city_state_list.pickle','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>votes_cool</th>\n",
       "      <th>votes_funny</th>\n",
       "      <th>votes_useful</th>\n",
       "      <th>cleaned_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84301</th>\n",
       "      <td>02ef18a93c6b829f0c78790ce5709a3887fcd139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9cee722c-5489-46a7-b0cf-4b2c61e6a527</td>\n",
       "      <td>4.0</td>\n",
       "      <td>So my friends and I came one night, bringing m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5291adc8-90e2-49ae-ad7b-57794f6c9a2c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[friend, came, one, night, bringing, good, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84302</th>\n",
       "      <td>02ef18a93c6b829f0c78790ce5709a3887fcd139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>773badd2-fd39-430a-ab32-22597ce1d76b</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Everyone has a little drag queen inside of the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5a921de7-ca35-4ca7-9125-4813cb88b1c2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[everyone, a-little, drag, queen, inside, pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84303</th>\n",
       "      <td>02ef18a93c6b829f0c78790ce5709a3887fcd139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ca8bee7-dd87-487e-bad1-2310f874bf5e</td>\n",
       "      <td>3.0</td>\n",
       "      <td>the venue itself is amazing.  it's huge! Sever...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10d6c7d4-c641-4db0-9f98-ca384ee69338</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[venue, amazing], [huge], [several, floor, eq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84304</th>\n",
       "      <td>02ef18a93c6b829f0c78790ce5709a3887fcd139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83512718-a8a5-4a87-9d07-7badab3e32ae</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Ooooo Ennis loved this place! S/he felt the co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deb75413-f53d-4f35-a403-d7d0048e2c97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[ooooo, loved, place], [felt, competition, ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84305</th>\n",
       "      <td>02ef18a93c6b829f0c78790ce5709a3887fcd139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83f4d507-9524-47d0-a337-7e67ba8093d5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Choosing a star rating for this is difficult a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67290bdd-9cfb-4200-9586-3e04d62b6e02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[choosing, star, rating, difficult, at-best],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    business_id date  \\\n",
       "84301  02ef18a93c6b829f0c78790ce5709a3887fcd139  NaN   \n",
       "84302  02ef18a93c6b829f0c78790ce5709a3887fcd139  NaN   \n",
       "84303  02ef18a93c6b829f0c78790ce5709a3887fcd139  NaN   \n",
       "84304  02ef18a93c6b829f0c78790ce5709a3887fcd139  NaN   \n",
       "84305  02ef18a93c6b829f0c78790ce5709a3887fcd139  NaN   \n",
       "\n",
       "                                  review_id stars  \\\n",
       "84301  9cee722c-5489-46a7-b0cf-4b2c61e6a527   4.0   \n",
       "84302  773badd2-fd39-430a-ab32-22597ce1d76b   5.0   \n",
       "84303  1ca8bee7-dd87-487e-bad1-2310f874bf5e   3.0   \n",
       "84304  83512718-a8a5-4a87-9d07-7badab3e32ae   3.0   \n",
       "84305  83f4d507-9524-47d0-a337-7e67ba8093d5   3.0   \n",
       "\n",
       "                                                    text type  \\\n",
       "84301  So my friends and I came one night, bringing m...  NaN   \n",
       "84302  Everyone has a little drag queen inside of the...  NaN   \n",
       "84303  the venue itself is amazing.  it's huge! Sever...  NaN   \n",
       "84304  Ooooo Ennis loved this place! S/he felt the co...  NaN   \n",
       "84305  Choosing a star rating for this is difficult a...  NaN   \n",
       "\n",
       "                                    user_id  votes_cool  votes_funny  \\\n",
       "84301  5291adc8-90e2-49ae-ad7b-57794f6c9a2c         NaN          NaN   \n",
       "84302  5a921de7-ca35-4ca7-9125-4813cb88b1c2         NaN          NaN   \n",
       "84303  10d6c7d4-c641-4db0-9f98-ca384ee69338         NaN          NaN   \n",
       "84304  deb75413-f53d-4f35-a403-d7d0048e2c97         NaN          NaN   \n",
       "84305  67290bdd-9cfb-4200-9586-3e04d62b6e02         NaN          NaN   \n",
       "\n",
       "       votes_useful                                  cleaned_tokenized  \n",
       "84301           NaN  [[friend, came, one, night, bringing, good, na...  \n",
       "84302           NaN  [[everyone, a-little, drag, queen, inside, pla...  \n",
       "84303           NaN  [[venue, amazing], [huge], [several, floor, eq...  \n",
       "84304           NaN  [[ooooo, loved, place], [felt, competition, ve...  \n",
       "84305           NaN  [[choosing, star, rating, difficult, at-best],...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deb75413-f53d-4f35-a403-d7d0048e2c97'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"deb75413-f53d-4f35-a403-d7d0048e2c97\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Drop 20% of the users from the dataset for testing\n",
    "user_set = list(set(review.user_id.values[:]))\n",
    "\n",
    "random.seed(0)\n",
    "shuffle(user_set) # Randomize \n",
    "n_users = float(len(user_set))\n",
    "\n",
    "user_set_training = user_set[:int(n_users*float(0.8))]\n",
    "with open('../output/training_users.pickle', 'wb') as f: \n",
    "    pickle.dump(user_set_training, f)\n",
    "    \n",
    "# Save a test set\n",
    "test_users = user_set[int(n_users*float(0.8)):]\n",
    "with open('../output/test_users.pickle', 'wb') as f: \n",
    "    pickle.dump(test_users, f)\n",
    "    \n",
    "# Make the active review set training only \n",
    "review = review[review.user_id.isin(user_set_training)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"deb75413-f53d-4f35-a403-d7d0048e2c97\" in user_set_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the documents by (i) business, (ii) users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fraction Processed 0.999822064057"
     ]
    }
   ],
   "source": [
    "# This is for review level not business level \n",
    "# docs = [\" \".join(list(chain.from_iterable(l))) for l in review.cleaned_tokenized.iloc[:]]\n",
    "\n",
    "n_reviews = -1 # all of them... \n",
    "# Flatten the reviews, so each review is just a single list of words.\n",
    "reviews_merged_bus = OrderedDict()\n",
    "business_set = set(review.business_id.values[:n_reviews])\n",
    "for i_bus, bus_id in enumerate(business_set):\n",
    "    if ((i_bus%2)==0):\n",
    "        print ('\\r Fraction Processed',float(i_bus+1)/len(business_set), end=\"\") \n",
    "    # This horrible line first collapses each review of a corresponding business into a list\n",
    "    # of lists, and then collapses the list of sentences to a long list of words\n",
    "    reviews_merged_bus[bus_id] = \" \".join(list(chain.from_iterable( \n",
    "                                    chain.from_iterable( review.cleaned_tokenized[review.business_id==bus_id] ))))    \n",
    "docs_bus = reviews_merged_bus.values()\n",
    "\n",
    "with open('../output/docs_bars_bus.pickle', 'wb') as f: \n",
    "    pickle.dump(docs_bus, f)\n",
    "\n",
    "with open('../output/bus_ids_bars_LDA.pickle', 'wb') as f: \n",
    "    pickle.dump(reviews_merged_bus.keys(), f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note that this section merges all reviews by the same person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fraction Processed 0.999817553761\n",
      "Merging Done...\n"
     ]
    }
   ],
   "source": [
    "# Flatten the reviews, so each review is just a single list of words.\n",
    "# reviews_merged_user = OrderedDict()\n",
    "\n",
    "# user_set = list(set(review.user_id.values[:n_reviews]))\n",
    "# n_users = float(len(user_set))\n",
    "# for i_user, user_id in enumerate(user_set[:]):\n",
    "#     if ((i_user%50)==0):\n",
    "#         print ('\\r Fraction Processed',float(i_user+1)/n_users, end=\"\") \n",
    "#     # This horrible line first collapses each review of a corresponding user reviews into a list\n",
    "#     # of lists, and then collapses the list of sentences to a long list of words\n",
    "#     reviews_merged_user[user_id] = \" \".join(list(chain.from_iterable( \n",
    "#                                     chain.from_iterable( review.cleaned_tokenized[review.user_id==user_id] ))))    \n",
    "# docs_users = reviews_merged_user.values()\n",
    "# print()\n",
    "# print(\"Merging Done...\")\n",
    "\n",
    "# with open('../output/docs_bars_users.pickle', 'wb') as f: \n",
    "#     pickle.dump(docs_users, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Flatten the reviews, so each review is just a single list of words.\n",
    "docs_reviews = [\" \".join(list(chain.from_iterable(rev))) for rev in review.cleaned_tokenized.values[:n_reviews]]\n",
    "\n",
    "with open('../output/docs_reviews.pickle', 'wb') as f: \n",
    "    pickle.dump(docs_reviews, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Across Bars and Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2 \n",
    "import sys\n",
    "sys.path.append('../vectorsearch/')\n",
    "import LDA\n",
    "reload(LDA)\n",
    "n_topics=30\n",
    "n_features=10000\n",
    "max_df=.75\n",
    "min_df=3\n",
    "max_iter=10\n",
    "alpha=6./n_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 14.924s.\n",
      "Fitting LDA models with tf features, n_samples=5620 and n_features=10000...\n"
     ]
    }
   ],
   "source": [
    "# Train the bar set over businesses\n",
    "#doc_users = pickle.load(open('../output/docs_bars_users.pickle', 'rb'))\n",
    "\n",
    "lda_bus = LDA.LDA(alpha=alpha, n_topics=n_topics, n_features=n_features, max_df=max_df, min_df=min_df, max_iter=max_iter,)\n",
    "lda_bus.vectorizecounts(docs_bus)\n",
    "lda_bus.fitLDA()\n",
    "LDA.SaveLDAModel('../output/LDA_model_bus.pickle', lda_bus)\n",
    "# Now can \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 10.863s.\n",
      "Fitting LDA models with tf features, n_samples=82216 and n_features=10000...\n",
      "done in 653.609s.\n"
     ]
    }
   ],
   "source": [
    "# Train the bar set over users\n",
    "\n",
    "# doc_users = pickle.load(open('../output/docs_bars_users.pickle', 'rb'))\n",
    "# lda_user = LDA.LDA(n_topics=n_topics, n_features=n_features, max_df=max_df, min_df=min_df, max_iter=max_iter,)\n",
    "# lda_user.vectorizecounts(docs_users)\n",
    "# lda_user.fitLDA()\n",
    "# LDA.SaveLDAModel('../output/LDA_model_user.pickle', lda_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 11.443s.\n",
      "Fitting LDA models with tf features, n_samples=186751 and n_features=10000...\n",
      "done in 496.987s.\n"
     ]
    }
   ],
   "source": [
    "# Train the bar set over users\n",
    "lda_reviews = pickle.load(open('../output/docs_reviews.pickle', 'rb'))\n",
    "lda_reviews = LDA.LDA(alpha=alpha, n_topics=n_topics, n_features=n_features, max_df=max_df, min_df=min_df, max_iter=max_iter,)\n",
    "lda_reviews.vectorizecounts(docs_reviews)\n",
    "lda_reviews.fitLDA()\n",
    "LDA.SaveLDAModel('../output/LDA_model_reviews.pickle', lda_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# doc_users = pickle.load(open('../output/docs_bars_users.pickle', 'rb'))\n",
    "# lda_user = LDA.LDA(n_topics=n_topics, n_features=n_features, max_df=max_df, min_df=min_df, max_iter=max_iter,)\n",
    "# lda_user.vectorizecounts(docs_users)\n",
    "# lda_user.fitLDA()\n",
    "# LDA.SaveLDAModel('../output/LDA_model_user.pickle', lda_user)\n",
    "\n",
    "\n",
    "#lda_bus.print_top_words(10)\n",
    "\n",
    "#.get_doc_topics(doc_users[10:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../vectorsearch/')\n",
    "import LDA\n",
    "\n",
    "bus_lda = LDA.LoadLDAModel('../output/LDA_model_bus.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The topic vector for a given business is given by this dataframe. \n",
    "bus_lda_ids = pickle.load(open('../output/bus_ids_bars_LDA.pickle', 'rb'))\n",
    "bus_vectors = pd.DataFrame()\n",
    "bus_vectors['business_id'] = bus_lda_ids\n",
    "transformed = bus_lda.lda.transform(bus_lda.tf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(transformed.shape)\n",
    "print(len(bus_vectors))\n",
    "\n",
    "bus_vectors['topic_vector'] = [bus_topic_vec for bus_topic_vec in transformed]\n",
    "normed_topic_vecs = map(lambda topic_vec: topic_vec/sqrt(np.dot(topic_vec, topic_vec)),\n",
    "                        bus_vectors.topic_vector) \n",
    "\n",
    "\n",
    "bus_vectors.topic_vector = normed_topic_vecs\n",
    "\n",
    "bus_vectors.to_pickle('../output/business_LDA_vectors.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualizationlda_reviews.get_doc_topics(doc_reviews[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pyLDAvis\n",
    "# import pandas as pd\n",
    "# import funcy as fp\n",
    "# from pyLDAvis import prepare as vis_prepare\n",
    "\n",
    "# def _extract_data(docs, vect, lda):\n",
    "#     #LDA scikit-learn implementation seems to have buggy code.\n",
    "#     #Topic_term_dists and doc_topic_dists isn't accummulated to 1.\n",
    "#     #Hence norm function implemented to normalize the distributions.\n",
    "#     norm = lambda data: pd.DataFrame(data).div(data.sum(1),axis=0).values\n",
    "#     vected = vect.fit_transform(docs)\n",
    "#     doc_topic_dists = norm(lda.fit_transform(vected))\n",
    "    \n",
    "#     return lda,vect, dict(\n",
    "#                       doc_lengths = docs.str.len(),\n",
    "#                       vocab = vect.get_feature_names(),\n",
    "#                       term_frequency = vected.sum(axis=0).tolist()[0],\n",
    "#                       topic_term_dists = norm(lda.components_),\n",
    "#                       doc_topic_dists = doc_topic_dists)\n",
    "\n",
    "# def prepare(docs, vect, lda, **kwargs):\n",
    "#     \"\"\"Create Prepared Data from sklearn's vectorizer and Latent Dirichlet\n",
    "#     Application.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     docs : Pandas Series.\n",
    "#         Documents to be passed as an input.\n",
    "#     vect : Scikit-Learn Vectorizer (CountVectorizer,TfIdfVectorizer).\n",
    "#         vectorizer to convert documents into matrix sparser\n",
    "#     lda  : sklearn.decomposition.LatentDirichletAllocation.\n",
    "#         Latent Dirichlet Allocation\n",
    "\n",
    "#     **kwargs: Keyword argument to be passed to pyLDAvis.prepare()\n",
    "\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     prepared_data : PreparedData\n",
    "#           the data structures used in the visualization\n",
    "\n",
    "\n",
    "#     Example\n",
    "#     --------\n",
    "#     For example usage please see this notebook:\n",
    "#     http://nbviewer.ipython.org/github/bmabey/pyLDAvis/blob/master/notebooks/sklearn.ipynb\n",
    "\n",
    "#     See\n",
    "#     ------\n",
    "#     See `pyLDAvis.prepare` for **kwargs.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     opts = fp.merge(_extract_data(docs, vect, lda)[2], kwargs)\n",
    "\n",
    "#     return vis_prepare(**opts)\n",
    "\n",
    "# vis_data = prepare(docs, tf_vectorizer, lda)\n",
    "\n",
    "# # \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../vectorsearch/')\n",
    "import LDA\n",
    "\n",
    "bus_lda = LDA.LoadLDAModel('../output/LDA_model_bus.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bus_lda.lda.n_jobs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
