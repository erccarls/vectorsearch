{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the word list in NLTK. \n",
    "We clean text in several stages.  Starting with a list of reviews:\n",
    "1. Divide review into sentences\n",
    "2. clean words (remove punctuation and extra characters)\n",
    "3. tokenize. \n",
    "4. multiword tokenize.\n",
    "5. remove stop words. \n",
    "6. Stem words \n",
    "6. words that occur under 3 times in the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cPickle as pickle\n",
    "# Load the yelp review data\n",
    "review = pd.read_pickle('../input/yelp_academic_dataset_review.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Adapted, but much improved from  ----   https://github.com/titipata/yelp_dataset_challenge\n",
    "\n",
    "\n",
    "import time\n",
    "import collections\n",
    "#import scipy.sparse as sp\n",
    "#import nltk.data\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from unidecode import unidecode\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "#from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "import sys\n",
    "sys.path.append('../vectorsearch/')\n",
    "from reverse_stemmer import SnowCastleStemmer\n",
    "import nltk\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "# tb_tokenizer = TreebankWordTokenizer()\n",
    "stops = set(stopwords.words(\"english\") + stopwords.words(\"spanish\"))\n",
    "keep_list = ['after', 'during', 'not', 'between', 'other', 'over', 'under', \n",
    "             'most', ' without', 'nor', 'no', 'very', 'against','don','aren']\n",
    "stops = set([word for word in stops if word not in keep_list])\n",
    "\n",
    "\n",
    "# Multiword tokenizer list taken from: \n",
    "# http://www.cs.cmu.edu/~ark/LexSem/\n",
    "# http://www.cs.cmu.edu/~ark/LexSem/STREUSLE2.1-mwes.tsv\n",
    "\n",
    "# This parses a list of multiword expressions from # http://www.cs.cmu.edu/~ark/LexSem/STREUSLE2.1-mwes.tsv\n",
    "# into NLTK format\n",
    "MWE = [] \n",
    "with open('../input/STREUSLE2.1-mwes.tsv') as f:\n",
    "    for line in f.readlines():\n",
    "        multiword_expression = line.split('\\t')[0].split()[1:]\n",
    "        MWE.append(multiword_expression)\n",
    "MWE_tokenizer = MWETokenizer(MWE, separator='-')\n",
    "# Add whatever additional custom multi-word-expressions.\n",
    "MWE_tokenizer.add_mwe(('dive', 'bar'))\n",
    "\n",
    "# Stemmer\n",
    "stemmer = SnowCastleStemmer(\"english\")\n",
    "wnl = WordNetLemmatizer()\n",
    "table = string.maketrans(\"\",\"\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and lower string\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : in string format\n",
    "    Returns\n",
    "    -------\n",
    "        text_clean : clean text input in string format\n",
    "    \"\"\"\n",
    "    return text.lower().translate(table, string.punctuation.replace('-',''))\n",
    "\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    1. Divide review into sentences\n",
    "    2. clean words\n",
    "    3. tokenize\n",
    "    4. multiword tokenize\n",
    "    5. remove stop words\n",
    "    6. stem words\n",
    "    Returns\n",
    "    ------\n",
    "        text_filtered: list of word in sentence\n",
    "    \"\"\"\n",
    "    # Splits into sentences.\n",
    "    sentence = sent_detector.tokenize(unidecode(text))\n",
    "    # Clean text: (remove) Remove extra puncuations marks...\n",
    "    text_clean = map(clean_text, sentence)\n",
    "\n",
    "    # Multiword expression tokenizer\n",
    "    text_tokenize = map(lambda x: whitespace_tokenizer.tokenize(x), text_clean)\n",
    "    text_tokenize = map(lambda x: MWE_tokenizer.tokenize(x), text_tokenize)\n",
    "    \n",
    "    # remove stop words\n",
    "    text_filtered = map(lambda x: [word for word in x if word not in stops], text_tokenize)\n",
    "    # lemmetize words (stemming removes too much...)\n",
    "#     text_stemmed = map(lambda x: [wnl.lemmatize(word) \n",
    "#                                   if wnl.lemmatize(word).endswith('e') \n",
    "#                                   else stemmer.stem(word) \n",
    "#                                   for word in x], text_filtered)\n",
    "    text_stemmed = map(lambda x: [wnl.lemmatize(word) for word in x], text_filtered)\n",
    "    #text_stemmed = map(lambda x: [stemmer.stem(word) for word in x], text_filtered)\n",
    "    return text_stemmed\n",
    "\n",
    "\n",
    "def unstem_text(text_stemmed):\n",
    "    '''\n",
    "    Unstem the text with the lowest count real word.  This helps readability.\n",
    "    '''\n",
    "    #unstem with the simplest word.  This helps readability of results...\n",
    "    text_unstemmed = map(lambda x: [stemmer.unstem(word)[0] \n",
    "                                  if len(stemmer.unstem(word))>0\n",
    "                                  else word\n",
    "                                  for word in x], text_stemmed)\n",
    "    return text_unstemmed\n",
    "    \n",
    "    \n",
    "def remove_low_occurence_words(texts, threshold=1): \n",
    "    '''\n",
    "    Remove words that appear fewer than \"threshold\" times.\n",
    "    '''\n",
    "    \n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for sentence in text:\n",
    "            for token in sentence:\n",
    "                 frequency[token] += 1\n",
    "    \n",
    "    texts = [[[token for token in sentence if frequency[token] > threshold]\n",
    "              for sentence in text] for text in texts]\n",
    "    return texts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bars (excluding restaurants) 4655\n",
      "Number of bar reviews 233041\n",
      "Cleaning and tokenizing\n"
     ]
    }
   ],
   "source": [
    "# Select reviews that correspond to the list of bars\n",
    "bar_ids = pickle.load(open('../output/bar_ids.pickle', 'r'))\n",
    "bar_reviews = review[review.business_id.isin(bar_ids)][:]\n",
    "print 'Number of bars (excluding restaurants)', len(bar_ids)\n",
    "print 'Number of bar reviews', np.sum(review.business_id.isin(bar_ids))\n",
    "\n",
    "# Clean and tokenize\n",
    "print 'Cleaning and tokenizing'\n",
    "review_sentences = map(clean_and_tokenize, bar_reviews.text.iloc[:])\n",
    "#review_sentences = map(unstem_text, review_sentences)\n",
    "\n",
    "# This is a list of reviews \n",
    "# each review contains a list of sentences\n",
    "# each sentence contains a list of words (tokens)\n",
    "review_sentences = remove_low_occurence_words(review_sentences, threshold=3)\n",
    "\n",
    "# They must be flattened for word2vec. \n",
    "# review_flatten = list(chain.from_iterable(review_sentences)) # This is the input to word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Append to df and save to file\n",
    "bar_reviews['cleaned_tokenized'] = review_sentences\n",
    "bar_reviews.to_pickle('../output/bar_reviews_cleaned_and_tokenized.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      "We checked this place out this past Monday for their wing night. We have heard that their wings are great and decided it was finally time to check it out. Their wings are whole wings and crispy, which is a nice change of pace. I got their wet Cajun sauce and garlic butter wings. The Cajun did not have a bold enough flavor for me and their sauce is too thin. The sauce was also thin for the garlic butter, but that is more expected. They were better than average, but I don't like seeing all the sauce resting at the bottom of the boat. I would definitely come try this place out again to sample some of the other items on the menu, but this will probably not become a regular stop for wings anytime soon.\n",
      "\n",
      "Tokenized\n",
      "[['checked', 'place', 'past', 'monday', 'wing-night'], ['heard', u'wing', 'great', 'decided', 'finally', 'time', 'check'], [u'wing', 'whole', u'wing', 'crispy', 'nice', 'change', 'pace'], ['got', 'wet', 'cajun', 'sauce', 'garlic', 'butter', u'wing'], ['cajun', 'not', 'bold', 'enough', 'flavor', 'sauce', 'thin'], ['sauce', 'also', 'thin', 'garlic', 'butter', 'expected'], ['better', 'average', 'dont', 'like', 'seeing', 'sauce', 'resting', 'bottom', 'boat'], ['would', 'definitely', 'come', 'try', 'place', 'sample', 'other', u'item', 'menu', 'probably', 'not', 'become', 'regular', 'stop', u'wing', 'anytime-soon']]\n"
     ]
    }
   ],
   "source": [
    "# Examine some samples....\n",
    "\n",
    "print 'Original'\n",
    "print bar_reviews['text'].iloc[1]\n",
    "print \n",
    "\n",
    "print 'Tokenized'\n",
    "print bar_reviews['cleaned_tokenized'].iloc[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
