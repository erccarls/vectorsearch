{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the word list in NLTK. \n",
    "We clean text in several stages.  Starting with a list of reviews:\n",
    "1. Divide review into sentences\n",
    "2. clean words (remove punctuation and extra characters)\n",
    "3. tokenize. \n",
    "4. multiword tokenize.\n",
    "5. remove stop words. \n",
    "6. Stem words \n",
    "6. words that occur under 3 times in the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Adapted, but much improved from  ----   https://github.com/titipata/yelp_dataset_challenge\n",
    "\n",
    "import re\n",
    "import time\n",
    "import collections\n",
    "#import scipy.sparse as sp\n",
    "#import nltk.data\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from unidecode import unidecode\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "#from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "# tb_tokenizer = TreebankWordTokenizer()\n",
    "stops = set(stopwords.words(\"english\") + stopwords.words(\"spanish\"))\n",
    "keep_list = ['after', 'during', 'not', 'between', 'other', 'over', 'under', \n",
    "             'most', ' without', 'nor', 'no', 'very', 'against','don','aren']\n",
    "stops = set([word for word in stops if word not in keep_list])\n",
    "\n",
    "\n",
    "# Multiword tokenizer list taken from: \n",
    "# http://www.cs.cmu.edu/~ark/LexSem/\n",
    "# http://www.cs.cmu.edu/~ark/LexSem/STREUSLE2.1-mwes.tsv\n",
    "\n",
    "# This parses a list of multiword expressions from # http://www.cs.cmu.edu/~ark/LexSem/STREUSLE2.1-mwes.tsv\n",
    "# into NLTK format\n",
    "MWE = [] \n",
    "with open('../input/STREUSLE2.1-mwes.tsv') as f:\n",
    "    for line in f.readlines():\n",
    "        multiword_expression = line.split('\\t')[0].split()[1:]\n",
    "        MWE.append(multiword_expression)\n",
    "MWE_tokenizer = MWETokenizer(MWE, separator='-')\n",
    "# Add whatever additional custom multi-word-expressions.\n",
    "MWE_tokenizer.add_mwe(('dive', 'bar'))\n",
    "# Stemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and lower string\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : in string format\n",
    "    Returns\n",
    "    -------\n",
    "        text_clean : clean text input in string format\n",
    "    \"\"\"\n",
    "    text_clean = re.sub(':', '', text.lower())\n",
    "    text_clean = re.sub(',', '', text_clean)\n",
    "    text_clean = re.sub('\\.', '', text_clean)\n",
    "    text_clean = re.sub('\\(', '', text_clean)\n",
    "    text_clean = re.sub('\\)', '', text_clean)\n",
    "    text_clean = re.sub('!', '', text_clean) \n",
    "    text_clean = re.sub('\\\\&', '', text_clean) \n",
    "    return text_clean\n",
    "\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    1. Divide review into sentences\n",
    "    2. clean words\n",
    "    3. tokenize\n",
    "    4. multiword tokenize\n",
    "    5. remove stop words\n",
    "    6. stem words\n",
    "    Returns\n",
    "    ------\n",
    "        text_filtered: list of word in sentence\n",
    "    \"\"\"\n",
    "    # Splits into sentences.\n",
    "    sentence = sent_detector.tokenize(unidecode(text))\n",
    "    # Clean text: (remove) Remove extra puncuations marks...\n",
    "    text_clean = map(clean_text, sentence)\n",
    "\n",
    "    # Multiword expression tokenizer\n",
    "    text_tokenize = map(lambda x: whitespace_tokenizer.tokenize(x), text_clean)\n",
    "    text_tokenize = map(lambda x: MWE_tokenizer.tokenize(x), text_tokenize)\n",
    "    \n",
    "    # remove stop words\n",
    "    text_filtered = map(lambda x: [word for word in x if word not in stops], text_tokenize)\n",
    "    # Stem words\n",
    "    text_stemmed = map(lambda x: [stemmer.stem(word) for word in x], text_filtered)\n",
    "    return text_stemmed\n",
    "\n",
    "\n",
    "def remove_low_occurence_words(texts, threshold=1): \n",
    "    '''\n",
    "    Remove words that appear fewer than \"threshold\" times.\n",
    "    '''\n",
    "    \n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for sentence in text:\n",
    "            for token in sentence:\n",
    "                 frequency[token] += 1\n",
    "    \n",
    "    texts = [[[token for token in sentence if frequency[token] > threshold]\n",
    "              for sentence in text] for text in texts]\n",
    "    return texts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cPickle as pickle\n",
    "# Load the yelp review data\n",
    "review = pd.read_pickle('../input/yelp_academic_dataset_review.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bars (excluding restaurants) 4655\n",
      "Number of bar reviews 233041\n",
      "Cleaning and tokenizing\n"
     ]
    }
   ],
   "source": [
    "# Select reviews that correspond to the list of bars\n",
    "bar_ids = pickle.load(open('../output/bar_ids.pickle', 'r'))\n",
    "bar_reviews = review[review.business_id.isin(bar_ids)]\n",
    "print 'Number of bars (excluding restaurants)', len(bar_ids)\n",
    "print 'Number of bar reviews', np.sum(review.business_id.isin(bar_ids))\n",
    "\n",
    "# Clean and tokenize\n",
    "print 'Cleaning and tokenizing'\n",
    "review_sentences = map(clean_and_tokenize, bar_reviews.text.iloc[:])\n",
    "\n",
    "# This is a list of reviews \n",
    "# each review contains a list of sentences\n",
    "# each sentence contains a list of words (tokens)\n",
    "review_sentences = remove_low_occurence_words(review_sentences, threshold=3)\n",
    "\n",
    "# They must be flattened for word2vec. \n",
    "# review_flatten = list(chain.from_iterable(review_sentences)) # This is the input to word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-c:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# with open('../output/bar_reviews_cleaned_and_tokenized.pickle', 'wb') as f:\n",
    "#     pickle.dump(review_sentences, f)\n",
    "\n",
    "bar_reviews['cleaned_tokenized'] = review_sentences\n",
    "bar_reviews.to_pickle('../output/bar_reviews_cleaned_and_tokenized.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      "We checked this place out this past Monday for their wing night. We have heard that their wings are great and decided it was finally time to check it out. Their wings are whole wings and crispy, which is a nice change of pace. I got their wet Cajun sauce and garlic butter wings. The Cajun did not have a bold enough flavor for me and their sauce is too thin. The sauce was also thin for the garlic butter, but that is more expected. They were better than average, but I don't like seeing all the sauce resting at the bottom of the boat. I would definitely come try this place out again to sample some of the other items on the menu, but this will probably not become a regular stop for wings anytime soon.\n",
      "\n",
      "Tokenized\n",
      "[['check', 'plac', 'past', 'monday', 'wing-night'], ['heard', 'wing', 'gre', 'decid', 'fin', 'tim', 'check'], ['wing', 'whol', 'wing', 'crispy', 'nic', 'chang', 'pac'], ['got', 'wet', 'cajun', 'sauc', 'garl', 'but', 'wing'], ['cajun', 'not', 'bold', 'enough', 'flav', 'sauc', 'thin'], ['sauc', 'also', 'thin', 'garl', 'but', 'expect'], ['bet', 'av', \"don't\", 'lik', 'see', 'sauc', 'rest', 'bottom', 'boat'], ['would', 'definit', 'com', 'try', 'plac', 'sampl', 'oth', 'item', 'menu', 'prob', 'not', 'becom', 'regul', 'stop', 'wing', 'anytime-soon']]\n"
     ]
    }
   ],
   "source": [
    "# Examine some samples....\n",
    "\n",
    "\n",
    "\n",
    "print 'Original'\n",
    "print bar_reviews['text'].iloc[1]\n",
    "print \n",
    "\n",
    "print 'Tokenized'\n",
    "print bar_reviews['cleaned_tokenized'].iloc[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'decid'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('decide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-72e382e11f9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mreverse_stemmer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSnowCastleStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mreverse_stemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSnowCastleStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/data/insight_yelp/vectorsearch/reverse_stemmer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stem_memory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;31m# switch stem and memstem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../vectorsearch/')\n",
    "from reverse_stemmer import SnowCastleStemmer\n",
    "\n",
    "reverse_stemmer = SnowCastleStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
