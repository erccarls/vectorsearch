{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load the bar review dataset \n",
    "review = pd.read_pickle('../output/bar_restaurant_reviews_cleaned_and_tokenized.pickle')\n",
    "review.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Doc: \n",
      "-----------------\n",
      "TaggedDocument(['food', 'great', 'best', 'thing', u'wing', u'wing', 'simply', 'fantastic', 'wet', 'cajun', 'best', 'most', 'popular', 'also', 'like', 'seasoned', 'salt', u'wing', 'wing-night', 'monday', 'wednesday', 'night', '075', 'whole', u'wing', 'dining', 'area', 'nice', 'very', 'family', 'friendly', 'bar', 'very', 'nice', 'well', 'place', 'truly', 'yinzers', 'dream', 'pittsburgh', 'dad', 'would', 'love', 'place', 'nat'], [u'Di3exaUCFNw1V4kSNW5pgA'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import gensim\n",
    "from itertools import chain\n",
    "import sys\n",
    "sys.path.append('../vectorsearch/')\n",
    "import nltk_helper\n",
    "import doc2vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "n_epochs = 10\n",
    "n_docs = -1 # -1 for almost all of them...\n",
    "\n",
    "# Collapse each review to a 1D list of words.\n",
    "review_flatten = [list(chain.from_iterable(doc)) for doc in review.cleaned_tokenized[:n_docs]]\n",
    "\n",
    "\n",
    "# Generate the tagged document list. \n",
    "docs = [TaggedDocument(words, [review.review_id.iloc[index]])\n",
    "                             for index, words in enumerate(review_flatten)]\n",
    "\n",
    "\n",
    "# A list of words for each review  \n",
    "sentences = [doc.words for doc in docs]\n",
    "\n",
    "print '\\nFirst Doc: \\n-----------------\\n', docs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec(min_count=5, window=8, size=100, sample=1e-4, negative=5, workers=12)\n",
    "# Build the vocab from list of sentences.\n",
    "model.build_vocab(docs) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10, alpha 0.0160\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "for epoch in range(10):\n",
    "    print '\\rTraining Epoch %i, alpha %1.4f'%(epoch+1, model.alpha),\n",
    "    #model.train(np.random.permutation(docs))\n",
    "    shuffle(docs)\n",
    "    model.train(docs)\n",
    "    model.alpha -= 0.001 # decrease the learning rate\n",
    "    model.min_alpha = model.alpha # fix the learning rate, no decay\n",
    "\n",
    "model.init_sims(replace=True)    \n",
    "# # Normalize the word vectors.\n",
    "# vec_norms = np.sqrt(np.sum(model.syn0**2, axis=1))\n",
    "# model.syn0 = (model.syn0/vec_norms[:, numpy.newaxis])\n",
    "# # Normalize the doc vectors.\n",
    "# vec_norms = np.sqrt(np.sum(model.docvecs.doctag_syn0**2, axis=1))\n",
    "# model.docvecs.doctag_syn0 = (model.docvecs.doctag_syn0/vec_norms[:, numpy.newaxis])\n",
    "\n",
    "model.save('../output/doc2vec_bars.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Thj-jeeY3rpu3dMJvTwTKg', 0.6003409624099731), (u'qpKFRaQvtrFRhnaQiR9I6g', 0.5944679975509644), (u'AkYXLQgc8OiHzh6srSTQCg', 0.58905029296875), (u'5Ipcu5mDhR__OPFXBJA_1g', 0.5879070162773132), (u'DNzrVCKa9jTKe9MQMxiXew', 0.5869959592819214), (u'CsBC1lyGlAcKxO1DI0XY6w', 0.5863107442855835), (u'quH_YU1n3NEHgUeo_IM5LQ', 0.5834709405899048), (u'IGL-pUN_vHdyZeqV24S-9Q', 0.5767616033554077), (u'VOhV6DvuwxnVpl6az7smmQ', 0.5720300674438477), (u'796ZD0lWIbHsipRfvZaDqQ', 0.571638822555542)] \n",
      "\n",
      "[('wine', 0.6038611531257629), (u'import', 0.5595511198043823), ('microbrews', 0.552147626876831), (u'draft', 0.5492744445800781), (u'craft', 0.5180332064628601), ('domestic', 0.5174193978309631), ('tap', 0.5162851810455322), ('chardonnay', 0.49968481063842773), ('whiskey', 0.48812004923820496), ('lager', 0.47685831785202026)] \n",
      "\n",
      "[(u'dayAuooCnXLB06lBpWtLyQ', 0.554182767868042), (u'_yNyxUfsLOfGOyneVxM9lw', 0.5318312644958496), (u'XeRek8yFWv3IN69BudilEQ', 0.5191066265106201), (u'J90iqydSFvYHArQ6lp1xxQ', 0.517946720123291), (u'46vczZARUL6g3B7o2HSgOw', 0.5173835754394531), (u'QwOPx5Q6VNSgLk_-c-v1Lw', 0.5105499625205994), (u'lw42UeqsvdZNgvvX2H5IpQ', 0.5085176825523376), (u'hJKSAPccjHNFaJG9Ng9dgg', 0.5013757944107056), (u'uQrmVccRGzqzxVkzwGOI7A', 0.4999310374259949), (u'9XPnFwj1cgyw1TZD3lt80Q', 0.4934437572956085)] \n",
      "\n",
      "[ u'My brother and I make the trek from N Scottsdale to The Drummer almost every weekend.   Jesse makes the HOTTEST suicide grilled wings on the planet - we love \\'em!  Service is great and the \"regulars\" are pretty friendly too.  Drink prices are good and there are plenty of TV\\'s.  One of the better \"dive bars\" in the area.']\n",
      "[('griffen', 0.46063828468322754), ('sympatique', 0.4249235987663269), ('chaine', 0.4021277129650116), ('stafford', 0.3905448913574219), ('yelpd', 0.38214969635009766), ('satisfaire', 0.381763756275177), ('discriminates', 0.3786201477050781), ('somedays', 0.3785707652568817), ('uniquement', 0.37743932008743286), ('mozz', 0.376831591129303)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can find similar documents..\n",
    "print model.docvecs.most_similar(positive=['KUinHkKyGhznElgIzx0yIw']), '\\n'\n",
    "\n",
    "# Can find similar words...Re: Dream Companies and contact from recruiters\n",
    "print model.most_similar(positive=['beer']), '\\n'\n",
    "\n",
    "# Can find documents that are most similar to keywords.... \n",
    "print model.docvecs.most_similar(positive=[model['beer'], model['music']]), '\\n'\n",
    "\n",
    "# Can find words that are most common in documents\n",
    "print review.text[review.review_id=='KUinHkKyGhznElgIzx0yIw'].values\n",
    "print model.most_similar(positive=[model.docvecs['KUinHkKyGhznElgIzx0yIw']]), '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
