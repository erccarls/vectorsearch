{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications. A survey of user opinion of computer system response time\",\n",
    "              \"A survey of user opinion of computer system response time\",\n",
    "              \"The EPS user interface management system\",\n",
    "              \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stop-words\n",
      "  Downloading stop-words-2015.2.23.1.tar.gz\n",
      "Building wheels for collected packages: stop-words\n",
      "  Running setup.py bdist_wheel for stop-words ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /home/carlson/.cache/pip/wheels/22/74/80/77275c2f9f2f1d9841b51e169a38985640a10fbd2711d10791\n",
      "Successfully built stop-words\n",
      "Installing collected packages: stop-words\n",
      "Successfully installed stop-words-2015.2.23.1\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 8.1.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get stop words from https://pypi.python.org/pypi/stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n"
     ]
    }
   ],
   "source": [
    "#!pip install stop-words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adapted from  ----   https://github.com/titipata/yelp_dataset_challenge\n",
    "\n",
    "# functions for preprocessing various fields of the raw data\n",
    "import re\n",
    "import time\n",
    "import collections\n",
    "import scipy.sparse as sp\n",
    "import nltk.data\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "from unidecode import unidecode\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "\n",
    "# Import stop-words for english\n",
    "# Import stop-words for spanish            \n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')\n",
    "stop_words += get_stop_words('spanish')\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "tb_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Divide review into sentence, clean words,\n",
    "    and tokenize it.\n",
    "    Returns\n",
    "    ------\n",
    "        text_tokenize: list of word in sentence\n",
    "    \"\"\"\n",
    "    # Splits into sentences.\n",
    "    sentence = sent_detector.tokenize(unidecode(text))\n",
    "    \n",
    "    \n",
    "    print text\n",
    "    print sentence\n",
    "    \n",
    "    text_clean = map(clean_tssext, sentence)\n",
    "    text_tokenize = map(lambda x: whitespace_tokenizer.tokenize(x), text_clean)\n",
    "    return text_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human machine interface for lab abc computer applications. A survey of user opinion of computer system response time\n",
      "['Human machine interface for lab abc computer applications.', 'A survey of user opinion of computer system response time']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'clean_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c8343401f8d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mclean_and_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-1ad13e5a9e5e>\u001b[0m in \u001b[0;36mclean_and_tokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mtext_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[0mtext_tokenize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mwhitespace_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'clean_text' is not defined"
     ]
    }
   ],
   "source": [
    "print clean_and_tokenize(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
