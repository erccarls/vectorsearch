{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load bar business ids, and load the review dataset into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "review = pd.read_pickle('../input/yelp_academic_dataset_review.pickle')\n",
    "review.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../yelp_dataset_challenge/')\n",
    "import yelp_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bars (excluding restaurants) 4655\n",
      "Number of bar reviews 233041\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bar_ids = pickle.load(open('../output/bar_ids.pickle', 'r'))\n",
    "# Select reviews that correspond to the list of bars\n",
    "bar_reviews = review[review.business_id.isin(bar_ids)]\n",
    "\n",
    "\n",
    "print 'Number of bars (excluding restaurants)', len(bar_ids)\n",
    "print 'Number of bar reviews', np.sum(review.business_id.isin(bar_ids))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breaking into sentence...\n",
      "trianing word2vec model...\n"
     ]
    }
   ],
   "source": [
    "yelp_review_sample = list(bar_reviews.text.iloc[:])\n",
    "model = yelp_util.create_word2vec_model(yelp_review_sample, ) # word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From  ----   https://github.com/titipata/yelp_dataset_challenge\n",
    "\n",
    "\n",
    "# functions for preprocessing various fields of the raw data\n",
    "import re\n",
    "import time\n",
    "import collections\n",
    "import scipy.sparse as sp\n",
    "import nltk.data\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "from unidecode import unidecode\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "\n",
    "\n",
    "\n",
    "__all__ = [\"taglist_to_matrix\",\n",
    "           \"create_word2vec_model\",\n",
    "           \"clear_tensorflow_graph\",\n",
    "           \"get_stream_seq\",\n",
    "           \"get_word_embedding\",\n",
    "           \"create_vocab\",\n",
    "           \"word2id\"\n",
    "           ]\n",
    "\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "tb_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "\n",
    "def taglist_to_matrix(taglist):\n",
    "    \"\"\"\n",
    "    This function\n",
    "    Args:\n",
    "        taglist: list of list of tags. For example, each element of the list is the list of tags of a business category:\n",
    "        [u'Doctors', u'Health & Medical']\n",
    "    Returns:\n",
    "        A sparse matrix num_docs x tags where element i, j has the counts of how many time tag j appear in document i\n",
    "    \"\"\"\n",
    "\n",
    "    all_tags = [w for doc in taglist for w in doc]\n",
    "    counter = collections.Counter(all_tags)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    # sparse matrix indices\n",
    "    i_indices = [doc_idx for doc_idx in range(len(taglist)) for _ in taglist[doc_idx]]\n",
    "    j_indices = [word_to_id[w] for doc_idx in range(len(taglist)) for w in taglist[doc_idx]]\n",
    "    data = [1]*len(all_tags)\n",
    "    m = sp.csc_matrix((data, (i_indices, j_indices)))\n",
    "    m.sum_duplicates()\n",
    "    return m\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and lower string\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : in string format\n",
    "    Returns\n",
    "    -------\n",
    "        text_clean : clean text input in string format\n",
    "    \"\"\"\n",
    "    text_clean = re.sub(':', '', text.lower())\n",
    "    text_clean = re.sub(',', '', text_clean)\n",
    "    text_clean = re.sub('\\.', '', text_clean)\n",
    "    return text_clean\n",
    "\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Divide review into sentence, clean words,\n",
    "    and tokenize it.\n",
    "    Returns\n",
    "    ------\n",
    "        text_tokenize: list of word in sentence\n",
    "    \"\"\"\n",
    "    sentence = sent_detector.tokenize(unidecode(text))\n",
    "    text_clean = map(clean_text, sentence)\n",
    "    text_tokenize = map(lambda x: whitespace_tokenizer.tokenize(x), text_clean)\n",
    "    return text_tokenize\n",
    "\n",
    "\n",
    "def clean_and_tokenize_word(text):\n",
    "    \"\"\"\n",
    "    Clean and divide text (review) into list of words\n",
    "    Returns\n",
    "    ------\n",
    "        text_clean: list of word in sentence\n",
    "    \"\"\"\n",
    "    if isinstance(text, list):\n",
    "        text_clean = map(clean_text, text)\n",
    "        text_tokenize = map(whitespace_tokenizer.tokenize, text_clean)\n",
    "    elif isinstance(text, basestring):\n",
    "        text_clean = clean_text(text)\n",
    "        text_tokenize = whitespace_tokenizer.tokenize(text_clean)\n",
    "    else:\n",
    "        text_tokenize = []\n",
    "    return text_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_stream_seq(review_list, word2vec_model):\n",
    "    \"\"\"\n",
    "    From review list and word2vec model,\n",
    "    generate output stream of output of review index\n",
    "    correspond to concatenated review list\n",
    "    \"\"\"\n",
    "    review_list_clean = clean_and_tokenize_word(review_list)\n",
    "    review_list_flatten = list(chain.from_iterable(review_list_clean))\n",
    "    review_words_stream = filter(lambda x: x is not None,\n",
    "                             map(lambda x: word2vec_model.vocab.get(x).index if x in word2vec_model.vocab else None,\n",
    "                                 review_list_flatten)\n",
    "                                 )\n",
    "    return review_words_stream\n",
    "\n",
    "\n",
    "def get_word_embedding(word2vec_model):\n",
    "    embeddings = word2vec_model.syn0\n",
    "    print 'Vocabulary size: ', embeddings.shape[0]\n",
    "    print 'Word vector dimension: ', embeddings.shape[1]\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def create_vocab(review_list):\n",
    "    \"\"\"\n",
    "    Create dictionary out of review list\n",
    "    ref: http://deeplearning.net/tutorial/lstm.html\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenized sentences\n",
    "    review_list = map(lambda x: x.lower(), review_list)\n",
    "    tksents = [tb_tokenizer.tokenize(review) for review in review_list]\n",
    "    print('Building dictionary..')\n",
    "    wordcount = dict()\n",
    "    for sent in tksents:\n",
    "        for w in sent:\n",
    "            if w.lower() not in wordcount:\n",
    "                wordcount[w.lower()] = 1\n",
    "            else:\n",
    "                wordcount[w.lower()] += 1\n",
    "\n",
    "    counts = wordcount.values()\n",
    "    keys = wordcount.keys()\n",
    "    sorted_idx = np.argsort(counts)[::-1]\n",
    "\n",
    "    worddict = dict()\n",
    "    for idx, ss in enumerate(sorted_idx):\n",
    "        worddict[keys[ss]] = idx+2  # leave 0 and 1 (UNK)\n",
    "\n",
    "    print(np.sum(counts), ' total words ', len(keys), ' unique words')\n",
    "    return worddict, tksents\n",
    "\n",
    "\n",
    "def word2id(tksents, dictionary):\n",
    "    seqs = [None] * len(tksents)\n",
    "    for idx, ss in enumerate(tksents):\n",
    "        seqs[idx] = [dictionary[w.lower()] if w.lower() \\\n",
    "                        in dictionary else 1 for w in ss]\n",
    "    return seqs\n",
    "\n",
    "\n",
    "def load_yelp_review(X, labels, nb_words=None, skip_top=10,\\\n",
    "                        maxlen=None, test_split=0.2, seed=113, oov_char=1):\n",
    "    '''\n",
    "        Preprocess and load Yelp Reviews word2id sequences and labels for\n",
    "        polarity analysis\n",
    "        nb_words : Maximum number of words to index, else assign oov_char\n",
    "        skip_top : Skip n top most common words\n",
    "        maxlen   : Maximum sequence length\n",
    "        oov_char : Out-Of-Vocabulary word id\n",
    "        test_split : Train-Test split\n",
    "        ref:https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py\n",
    "    '''\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(X)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(labels)\n",
    "\n",
    "    if maxlen:\n",
    "        new_X = []\n",
    "        new_labels = []\n",
    "        for x, y in zip(X, labels):\n",
    "            if len(x) < maxlen:\n",
    "                new_X.append(x)\n",
    "                new_labels.append(y)\n",
    "        X = new_X\n",
    "        labels = new_labels\n",
    "\n",
    "    if not nb_words:\n",
    "        nb_words = max([max(x) for x in X])\n",
    "\n",
    "    if oov_char is not None:\n",
    "        X = [[oov_char if (w >= nb_words or w < skip_top) else w for w in x] for x in X]\n",
    "    else:\n",
    "        nX = []\n",
    "        for x in X:\n",
    "            nx = []\n",
    "            for w in x:\n",
    "                if (w >= nb_words or w < skip_top):\n",
    "                    nx.append(w)\n",
    "            nX.append(nx)\n",
    "        X = nX\n",
    "\n",
    "    X_train = X[:int(len(X)*(1-test_split))]\n",
    "    y_train = labels[:int(len(X)*(1-test_split))]\n",
    "\n",
    "    X_test = X[int(len(X)*(1-test_split)):]\n",
    "    y_test = labels[int(len(X)*(1-test_split)):]\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breaking into sentence...\n",
      "training word2vec model...\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "def create_vector_model(model, review_list, **kwargs):\n",
    "    \"\"\"\n",
    "    Create gensim Word2Vec model out of review list\n",
    "    where each element contains review\n",
    "    \"\"\"\n",
    "    print 'breaking into sentence...'\n",
    "    review_sentence = map(clean_and_tokenize, review_list)\n",
    "    review_flatten = list(chain.from_iterable(review_sentence))\n",
    "    print 'training word2vec model...'\n",
    "    vec_model = model(review_flatten,**kwargs)\n",
    "    return vec_model\n",
    "\n",
    "\n",
    "model_args = {'size':200, 'window':5, 'min_count':10, 'workers':12}\n",
    "word2vec_model = create_vector_model(model=gensim.models.Word2Vec, review_list=yelp_review_sample, **model_args)\n",
    "\n",
    "# model_args = {'num_topics':100}\n",
    "# lda_model = create_vector_model(model=gensim.models.LdaModel, review_list=yelp_review_sample, **model_args)\n",
    "\n",
    "\n",
    "\n",
    "#model.similarity('bar')\n",
    "#model.most_similar('bar', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from gensim import corpora, models, similarities\n",
    "# model = models.ldamodel.LdaModel(yelp_review_sample, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the food is great here. But the best thing they have is their wings. Their wings are simply fantastic!!  The \"Wet Cajun\" are by the best & most popular.  I also like the seasoned salt wings.  Wing Night is Monday & Wednesday night, $0.75 whole wings!\n",
      "\n",
      "The dining area is nice. Very family friendly! The bar is very nice is well.  This place is truly a Yinzer's dream!!  \"Pittsburgh Dad\" would love this place n'at!!\n"
     ]
    }
   ],
   "source": [
    "print yelp_review_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spicy', 0.6386415958404541),\n",
       " ('margarita', 0.619797945022583),\n",
       " ('smooth', 0.5970010161399841),\n",
       " ('guinness', 0.5922343730926514),\n",
       " ('creamy', 0.5905068516731262),\n",
       " ('yummy', 0.585984468460083),\n",
       " ('tasty', 0.583294153213501),\n",
       " ('thick', 0.5832158327102661),\n",
       " ('scotch', 0.583088219165802),\n",
       " ('dressing', 0.5767841339111328)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar(positive=['beer','sweet'])\n",
    "\n",
    "\n",
    "#bar_word = word2vec_model.vocab['bar']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 23.743s.\n",
      "Extracting tf features for LDA...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-ad2d84a2d83a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Use tf (raw term count) features for LDA.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Extracting tf features for LDA...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n\u001b[0m\u001b[0;32m     17\u001b[0m                                 stop_words='english')\n\u001b[0;32m     18\u001b[0m \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_features' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, #max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "\n",
    "from time import time\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(yelp_review_sample)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "print(\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.fit(yelp_review_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
