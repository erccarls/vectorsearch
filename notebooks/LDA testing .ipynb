{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load the bar review dataset \n",
    "review = pd.read_pickle('../output/bar_reviews_cleaned_and_tokenized.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food great best thing wing wing simply fantastic wet cajun best most popular also like seasoned salt wing wing-night monday wednesday night 075 whole wing dining area nice very family friendly bar very nice well place truly yinzers dream pittsburgh dad would love place nat\n",
      "checked place past monday wing-night heard wing great decided finally time check wing whole wing crispy nice change pace got wet cajun sauce garlic butter wing cajun not bold enough flavor sauce thin sauce also thin garlic butter expected better average dont like seeing sauce resting bottom boat would definitely come try place sample other item menu probably not become regular stop wing anytime-soon\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "docs = [\" \".join(list(chain.from_iterable(l))) for l in review.cleaned_tokenized.iloc[:]]\n",
    "\n",
    "print(docs[0])\n",
    "print(docs[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 0.077s.\n",
      "Fitting LDA models with tf features, n_samples=1000 and n_features=5000...\n",
      "done in 2.394s.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_samples = 1000\n",
    "n_features = 5000\n",
    "n_topics = 20\n",
    "n_top_words = 10\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.75, min_df=2, max_features=n_features)\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(docs[:n_samples])\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "print(\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online', learning_offset=10.,\n",
    "                                random_state=0, n_jobs=6)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "sing factory alexions request musical lyric 19 send peaceful laugh please wowed primantis oldest once closer union talent pleased recall\n",
      "Topic #1:\n",
      "aint humor rankin rick hosted requested worse search emils beer sing great good place awesome shot play something know buck\n",
      "Topic #2:\n",
      "daughter tea ice mango slider cool father strip mashed lobster kitchen dude enchilada episode kobe repeatedly bash semi piano player\n",
      "Topic #3:\n",
      "repeating quote driver bachelorette ashley performer mirror theme joke blast butt bitch pianist funny age wrote absolute acoustic talented ran\n",
      "Topic #4:\n",
      "sing song piano rock brewery bottom cab park dueling quarter band north he drum hit homestead shout arcade drive largest\n",
      "Topic #5:\n",
      "place drink food time to great back not get of night one go no good bar went like service little\n",
      "Topic #6:\n",
      "trip salad meat bar reuben kind of one dinner chicken basic goodness not first back run piece sitting literally in\n",
      "Topic #7:\n",
      "horrible fish another da good reuben could one very menu table want see service friend looked beer like neighborhood time\n",
      "Topic #8:\n",
      "brunch potato appetizer entree pancake egg scallop pretzel sausage food very salmon cheese smoked best casbah spinach good bread not\n",
      "Topic #9:\n",
      "candle medium overrated rockys everyday website beef strong advance corned roast bos 2000 occurred exist bud error rye lighter checked\n",
      "Topic #10:\n",
      "reuben sandwich fish syrup affordable basic pan trip song properly schnitzel fried wiener chicken breaded answer recommendation bread bag accurate\n",
      "Topic #11:\n",
      "beer good not food place to bar like great one very get go selection really menu also of would time\n",
      "Topic #12:\n",
      "dog hot beer chicago cheese wing cave chili pack sauce come mason sauerkraut mustard hotdog german six schnitzel spicy cheddar\n",
      "Topic #13:\n",
      "wing sauce cajun thin butter anytime garlic bottom page direction resting wet map boat reuben call recommend wonderful become monday\n",
      "Topic #14:\n",
      "corporate superb mom pop perfection bachelorette dave buster equally mess allowed fish shrimp chip non fried thanks to rock ever\n",
      "Topic #15:\n",
      "alexions bluegrass skinny mug circle satisfy hipster primantis trick attracts fish jam commercial mega trivia join stadium sandwich described wood\n",
      "Topic #16:\n",
      "casbah cheese chicken good delicious ordered pasta dessert restaurant salad really meal drink not service nice also bread big dish\n",
      "Topic #17:\n",
      "bomb shower mill irish casing rootbeer whatsoever everyones gob said win lindemans jerk house absurd buffet cleanest surly waste banquet\n",
      "Topic #18:\n",
      "sing piano song performer james required taylor musician cougar request guy birthday stage sang crawl hell celebrating coors considering microbrewed\n",
      "Topic #19:\n",
      "sing crowd awesome comedy year piano place peanut interactive ruben smaller shot musical bring again get fat wanting gone last\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 20)\n",
    "\n",
    "#lda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6dcbdbb2d26e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mvis_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-6dcbdbb2d26e>\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(docs, vect, lda, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-6dcbdbb2d26e>\u001b[0m in \u001b[0;36m_extract_data\u001b[1;34m(docs, vect, lda)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     return lda,vect, dict(\n\u001b[1;32m---> 15\u001b[1;33m                       \u001b[0mdoc_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                       \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                       \u001b[0mterm_frequency\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvected\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'str'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pandas as pd\n",
    "import funcy as fp\n",
    "from pyLDAvis import prepare as vis_prepare\n",
    "\n",
    "def _extract_data(docs, vect, lda):\n",
    "    #LDA scikit-learn implementation seems to have buggy code.\n",
    "    #Topic_term_dists and doc_topic_dists isn't accummulated to 1.\n",
    "    #Hence norm function implemented to normalize the distributions.\n",
    "    norm = lambda data: pd.DataFrame(data).div(data.sum(1),axis=0).values\n",
    "    vected = vect.fit_transform(docs)\n",
    "    doc_topic_dists = norm(lda.fit_transform(vected))\n",
    "    \n",
    "    return lda,vect, dict(\n",
    "                      doc_lengths = docs.str.len(),\n",
    "                      vocab = vect.get_feature_names(),\n",
    "                      term_frequency = vected.sum(axis=0).tolist()[0],\n",
    "                      topic_term_dists = norm(lda.components_),\n",
    "                      doc_topic_dists = doc_topic_dists)\n",
    "\n",
    "def prepare(docs, vect, lda, **kwargs):\n",
    "    \"\"\"Create Prepared Data from sklearn's vectorizer and Latent Dirichlet\n",
    "    Application.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs : Pandas Series.\n",
    "        Documents to be passed as an input.\n",
    "    vect : Scikit-Learn Vectorizer (CountVectorizer,TfIdfVectorizer).\n",
    "        vectorizer to convert documents into matrix sparser\n",
    "    lda  : sklearn.decomposition.LatentDirichletAllocation.\n",
    "        Latent Dirichlet Allocation\n",
    "\n",
    "    **kwargs: Keyword argument to be passed to pyLDAvis.prepare()\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    prepared_data : PreparedData\n",
    "          the data structures used in the visualization\n",
    "\n",
    "\n",
    "    Example\n",
    "    --------\n",
    "    For example usage please see this notebook:\n",
    "    http://nbviewer.ipython.org/github/bmabey/pyLDAvis/blob/master/notebooks/sklearn.ipynb\n",
    "\n",
    "    See\n",
    "    ------\n",
    "    See `pyLDAvis.prepare` for **kwargs.\n",
    "    \"\"\"\n",
    "    \n",
    "    opts = fp.merge(_extract_data(docs, vect, lda)[2], kwargs)\n",
    "\n",
    "    return vis_prepare(**opts)\n",
    "\n",
    "vis_data = prepare(docs, tf_vectorizer, lda)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 1.158s.\n",
      "Fitting the NMF model with tf-idf features,n_samples=100000 and n_features=10000...\n",
      "done in 3.671s.\n",
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "great food atmosphere beer wine selection service drink place price time music experience staff bar special happy always hour date\n",
      "Topic #1:\n",
      "place love really like nice fun awesome cool favorite out friend hang to watch date game amazing looking absolutely recommend\n",
      "Topic #2:\n",
      "service very friendly staff nice always excellent attentive slow atmosphere server quick bartender awesome fast customer and helpful waitress super\n",
      "Topic #3:\n",
      "good food pretty really beer price drink selection very overall always decent amazing time atmosphere burger wine special fry pizza\n",
      "Topic #4:\n",
      "not bar time like one get drink beer really of night im best ive bad no in dont well the\n",
      "Topic #5:\n",
      "go back to definitely would want have cant come wait ill eat dont never try going id next get wrong\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "\n",
    "n_samples = 100000\n",
    "n_features = 10000\n",
    "n_topics = 6\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "# # Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features=n_features)\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(review_flatten[:n_samples])\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model with tf-idf features,\"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
